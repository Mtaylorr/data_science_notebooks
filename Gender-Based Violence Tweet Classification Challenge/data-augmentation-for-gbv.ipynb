{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install gdown","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-20T08:09:41.482775Z","iopub.execute_input":"2021-10-20T08:09:41.48303Z","iopub.status.idle":"2021-10-20T08:10:01.838969Z","shell.execute_reply.started":"2021-10-20T08:09:41.483003Z","shell.execute_reply":"2021-10-20T08:10:01.838143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown \nurl = 'https://drive.google.com/uc?export=download&id=1zFIVT5wKEmwiasBafjDlqdZF92Gi1blA' \noutput = 'GBV.zip'\ngdown.download(url, output)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:01.842069Z","iopub.execute_input":"2021-10-20T08:10:01.842375Z","iopub.status.idle":"2021-10-20T08:10:06.436338Z","shell.execute_reply.started":"2021-10-20T08:10:01.842337Z","shell.execute_reply":"2021-10-20T08:10:06.43541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! unzip  GBV.zip","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:06.437871Z","iopub.execute_input":"2021-10-20T08:10:06.438325Z","iopub.status.idle":"2021-10-20T08:10:07.240334Z","shell.execute_reply.started":"2021-10-20T08:10:06.438284Z","shell.execute_reply":"2021-10-20T08:10:07.239511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport random\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch \nimport warnings\nfrom tqdm import tqdm_notebook\n\nwarnings.filterwarnings(\"ignore\")\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.242985Z","iopub.execute_input":"2021-10-20T08:10:07.243492Z","iopub.status.idle":"2021-10-20T08:10:07.252166Z","shell.execute_reply.started":"2021-10-20T08:10:07.243433Z","shell.execute_reply":"2021-10-20T08:10:07.251328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"Train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.254001Z","iopub.execute_input":"2021-10-20T08:10:07.254411Z","iopub.status.idle":"2021-10-20T08:10:07.408148Z","shell.execute_reply.started":"2021-10-20T08:10:07.254374Z","shell.execute_reply":"2021-10-20T08:10:07.407421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"Test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.409544Z","iopub.execute_input":"2021-10-20T08:10:07.409875Z","iopub.status.idle":"2021-10-20T08:10:07.469146Z","shell.execute_reply.started":"2021-10-20T08:10:07.409838Z","shell.execute_reply":"2021-10-20T08:10:07.468478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idtoclass = df.type.unique()\nclasstoid = {idtoclass[i]:i for i in range(len(idtoclass))}\nprint(idtoclass)\nprint(classtoid)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.470549Z","iopub.execute_input":"2021-10-20T08:10:07.470813Z","iopub.status.idle":"2021-10-20T08:10:07.488905Z","shell.execute_reply.started":"2021-10-20T08:10:07.470777Z","shell.execute_reply":"2021-10-20T08:10:07.488162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"type\"] = df[\"type\"].apply(lambda x:classtoid[x])","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.490129Z","iopub.execute_input":"2021-10-20T08:10:07.490471Z","iopub.status.idle":"2021-10-20T08:10:07.520259Z","shell.execute_reply.started":"2021-10-20T08:10:07.490413Z","shell.execute_reply":"2021-10-20T08:10:07.519619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classcount = {i:len(df[df.type==i]) for i in range(len(idtoclass))}\nprint(classcount)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.522132Z","iopub.execute_input":"2021-10-20T08:10:07.522896Z","iopub.status.idle":"2021-10-20T08:10:07.536386Z","shell.execute_reply.started":"2021-10-20T08:10:07.522859Z","shell.execute_reply":"2021-10-20T08:10:07.535529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.539259Z","iopub.execute_input":"2021-10-20T08:10:07.53953Z","iopub.status.idle":"2021-10-20T08:10:07.554122Z","shell.execute_reply.started":"2021-10-20T08:10:07.539495Z","shell.execute_reply":"2021-10-20T08:10:07.553322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk import word_tokenize\nimport string\nnltk.download(\"punkt\")\ndef remove_punct(s):\n  s = list(s.split(\" \"))\n  s = \" \".join(s)\n  \n  s = [s for s in word_tokenize(s) if s not in string.punctuation]\n  s = \" \".join(s)\n  return s\n  ","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.555449Z","iopub.execute_input":"2021-10-20T08:10:07.55572Z","iopub.status.idle":"2021-10-20T08:10:07.617053Z","shell.execute_reply.started":"2021-10-20T08:10:07.555687Z","shell.execute_reply":"2021-10-20T08:10:07.616366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removing punctuation","metadata":{}},{"cell_type":"code","source":"df[\"tweet\"] = df[\"tweet\"].apply(remove_punct)\ntest_df[\"tweet\"] = test_df[\"tweet\"].apply(remove_punct)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:07.618521Z","iopub.execute_input":"2021-10-20T08:10:07.618782Z","iopub.status.idle":"2021-10-20T08:10:36.63575Z","shell.execute_reply.started":"2021-10-20T08:10:07.618748Z","shell.execute_reply":"2021-10-20T08:10:36.634989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lowering the data","metadata":{}},{"cell_type":"code","source":"test_df[\"tweet\"] = test_df[\"tweet\"].apply(lambda x: x.lower())\ndf[\"tweet\"] = df[\"tweet\"].apply(lambda x: x.lower())","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:36.63717Z","iopub.execute_input":"2021-10-20T08:10:36.63744Z","iopub.status.idle":"2021-10-20T08:10:36.694436Z","shell.execute_reply.started":"2021-10-20T08:10:36.637403Z","shell.execute_reply":"2021-10-20T08:10:36.693717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:36.695564Z","iopub.execute_input":"2021-10-20T08:10:36.695819Z","iopub.status.idle":"2021-10-20T08:10:36.705139Z","shell.execute_reply.started":"2021-10-20T08:10:36.695785Z","shell.execute_reply":"2021-10-20T08:10:36.704443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv(\"test_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:36.70659Z","iopub.execute_input":"2021-10-20T08:10:36.707084Z","iopub.status.idle":"2021-10-20T08:10:36.863252Z","shell.execute_reply.started":"2021-10-20T08:10:36.707046Z","shell.execute_reply":"2021-10-20T08:10:36.862507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentation with synonym replacement ","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download(\"wordnet\")\nfrom nltk.corpus import wordnet\n\nAUGMENTATION_PROB = 0.3\ndef getsynonym(word):\n  if  random.random() > AUGMENTATION_PROB : \n    return word\n  syns = wordnet.synsets(word)\n  synonyms = set()\n  synonyms.add(word)\n  for syn in syns : \n    for name in syn.lemma_names():\n      synonyms.add(name)\n\n  synonyms = list(synonyms)\n  return random.choice(synonyms)\n\ndef augment(s):\n  s = list(s.split())\n  s = [getsynonym(x) for x in s]\n\n  s = \" \".join(s)\n  return s  \n","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:36.864387Z","iopub.execute_input":"2021-10-20T08:10:36.864658Z","iopub.status.idle":"2021-10-20T08:10:36.87367Z","shell.execute_reply.started":"2021-10-20T08:10:36.864623Z","shell.execute_reply":"2021-10-20T08:10:36.872939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Min_SAMPLES  = 30000\ndf_augmented = df.copy().drop(\"Tweet_ID\", axis=1)\nfor i in range(len(idtoclass)):\n    texts = list(df[df.type==i].tweet)\n    missing = Min_SAMPLES - classcount[i]\n    for _ in range(missing):\n        txt = random.choice(texts)\n        txt = augment(txt)\n        d = {\"tweet\":txt, \"type\":i }\n        df_augmented = df_augmented.append(d, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:10:36.874938Z","iopub.execute_input":"2021-10-20T08:10:36.875811Z","iopub.status.idle":"2021-10-20T08:16:46.060135Z","shell.execute_reply.started":"2021-10-20T08:10:36.875769Z","shell.execute_reply":"2021-10-20T08:16:46.059115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_augmented.to_csv(\"synoym_30000.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:16:46.070272Z","iopub.execute_input":"2021-10-20T08:16:46.070703Z","iopub.status.idle":"2021-10-20T08:16:47.424079Z","shell.execute_reply.started":"2021-10-20T08:16:46.070666Z","shell.execute_reply":"2021-10-20T08:16:47.423331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentation using transformers Based on this blog : https://towardsdatascience.com/nlp-data-augmentation-using-transformers-89a44a993bab","metadata":{}},{"cell_type":"code","source":"class  Augment:\n    def __init__(self, augmentations, probs=None):\n        self.augmentations = augmentations\n        if probs is None : \n            self.probs = np.ones((len(augmentations)))/len(augmentations)\n        else : \n            self.probs = probs\n    \n    def augment(self,text): \n        augmentation = np.random.choice(self.augmentations, p=probs)\n        return augmentation.augment(text)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T08:16:46.061871Z","iopub.execute_input":"2021-10-20T08:16:46.062134Z","iopub.status.idle":"2021-10-20T08:16:46.067778Z","shell.execute_reply.started":"2021-10-20T08:16:46.062097Z","shell.execute_reply":"2021-10-20T08:16:46.067108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Translation : \n    def __init__(self):\n        \n        #English to German using the Pipeline and T5\n        self.translator_en_to_de = pipeline(\"translation_en_to_de\", model='t5-base',device=0)\n\n        #Germal to English using Bert2Bert model\n        self.tokenizer = AutoTokenizer.from_pretrained(\"google/bert2bert_L-24_wmt_de_en\", pad_token=\"<pad>\", eos_token=\"</s>\", bos_token=\"<s>\")\n        self.model_de_to_en = AutoModelForSeq2SeqLM.from_pretrained(\"google/bert2bert_L-24_wmt_de_en\")\n        \n    def augment(self, text):\n        en_to_de_output = self.translator_en_to_de(text)\n        translated_text = en_to_de_output[0]['translation_text']\n        \n        input_ids = self.tokenizer(translated_text, return_tensors=\"pt\", add_special_tokens=False).input_ids\n        output_ids = self.model_de_to_en.generate(input_ids)[0]\n        augmented_text = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n        return augmented_text        ","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:34:47.499707Z","iopub.execute_input":"2021-10-14T19:34:47.500254Z","iopub.status.idle":"2021-10-14T19:34:47.507284Z","shell.execute_reply.started":"2021-10-14T19:34:47.500212Z","shell.execute_reply":"2021-10-14T19:34:47.506352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Insertion : \n    def __init__(self):\n        \n        self.unmasker = pipeline('fill-mask', model='bert-base-cased',device=0)\n        #I went to see a new movie in the theater\n        \n    def augment(self, text):\n        orig_text_list = text.split()\n        len_input = len(orig_text_list)\n        #Random index where we want to insert the word except at the start or end\n        rand_idx = random.randint(1,len_input-2)\n\n        new_text_list = orig_text_list[:rand_idx] + ['[MASK]'] + orig_text_list[rand_idx:]\n        new_mask_sent = ' '.join(new_text_list)\n        \n        #I went to see a [Mask] movie in the theater\n\n        augmented_text_list = self.unmasker(new_mask_sent)\n        augmented_text = augmented_text_list[0]['sequence']\n        return augmented_text        ","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:34:49.003081Z","iopub.execute_input":"2021-10-14T19:34:49.00362Z","iopub.status.idle":"2021-10-14T19:34:49.010903Z","shell.execute_reply.started":"2021-10-14T19:34:49.003577Z","shell.execute_reply":"2021-10-14T19:34:49.010117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Replacement : \n    def __init__(self):\n        \n        self.unmasker = pipeline('fill-mask', model='bert-base-cased',device=0)\n        #I went to see a new movie in the theater\n        \n    def augment(self, text):\n        orig_text_list = text.split()\n        len_input = len(orig_text_list)\n        #Random index where we want to replace the word \n        rand_idx = random.randint(1,len_input-1)\n        orig_word = orig_text_list[rand_idx]\n        new_text_list = orig_text_list.copy()\n        new_text_list[rand_idx] = '[MASK]'\n        new_mask_sent = ' '.join(new_text_list)\n        #I went to [MASK] a movie in the theater\n        augmented_text_list = self.unmasker(new_mask_sent)\n        #To ensure new word and old word are not name\n        for res in augmented_text_list:\n          if res['token_str'] != orig_word:\n            augmented_text = res['sequence']\n            break\n        #I went to watch a movie in the theater\n        return augmented_text   ","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:34:55.73086Z","iopub.execute_input":"2021-10-14T19:34:55.731135Z","iopub.status.idle":"2021-10-14T19:34:55.73798Z","shell.execute_reply.started":"2021-10-14T19:34:55.731107Z","shell.execute_reply":"2021-10-14T19:34:55.737024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generation : \n    def __init__(self):\n        \n        self.generator =  pipeline('text-generation', model='gpt2', device=1)\n        #I went to see a new movie in the theater\n        \n    def augment(self, text):\n        input_length = len(text.split())\n        num_new_words = 5\n        output_length = input_length + num_new_words\n        gpt_output = self.generator(text, max_length=output_length, num_return_sequences=5)\n        augmented_text = gpt_output[0]['generated_text']\n        #I went to see a movie in the theater, and the director was\n        return augmented_text ","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:34:56.243345Z","iopub.execute_input":"2021-10-14T19:34:56.24398Z","iopub.status.idle":"2021-10-14T19:34:56.250124Z","shell.execute_reply.started":"2021-10-14T19:34:56.243942Z","shell.execute_reply":"2021-10-14T19:34:56.249224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmentations = [Replacement()] #[Translation(), Insertion(), Replacement()]\nprobs = [1.0]#[0.40,0.30,0.30]","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:36:20.045979Z","iopub.execute_input":"2021-10-14T19:36:20.04654Z","iopub.status.idle":"2021-10-14T19:36:46.470426Z","shell.execute_reply.started":"2021-10-14T19:36:20.0465Z","shell.execute_reply":"2021-10-14T19:36:46.469535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"augmenter = Augment(augmentations, probs)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:36:46.472237Z","iopub.execute_input":"2021-10-14T19:36:46.472529Z","iopub.status.idle":"2021-10-14T19:36:46.478824Z","shell.execute_reply.started":"2021-10-14T19:36:46.472449Z","shell.execute_reply":"2021-10-14T19:36:46.478095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"I'am here and i want to go the cinema\"\n\nfor i in range(20):\n    print(augmenter.augment(text))","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:36:46.480332Z","iopub.execute_input":"2021-10-14T19:36:46.480671Z","iopub.status.idle":"2021-10-14T19:36:47.475873Z","shell.execute_reply.started":"2021-10-14T19:36:46.480632Z","shell.execute_reply":"2021-10-14T19:36:47.475127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Min_SAMPLES  = 30000\ndf_augmented = df.copy().drop(\"Tweet_ID\", axis=1)\nfor i in range(len(idtoclass)):\n    texts = list(df[df.type==i].tweet)\n    missing = Min_SAMPLES - classcount[i]\n    print(f\"[Class {i}]\")\n    for _ in tqdm_notebook(range(missing)):\n        txt = random.choice(texts)\n        txt = augmenter.augment(txt)\n        d = {\"tweet\":txt, \"type\":i }\n        df_augmented = df_augmented.append(d, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:37:36.078444Z","iopub.execute_input":"2021-10-14T19:37:36.078762Z","iopub.status.idle":"2021-10-14T20:05:54.214268Z","shell.execute_reply.started":"2021-10-14T19:37:36.078728Z","shell.execute_reply":"2021-10-14T20:05:54.213336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_augmented.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T20:05:54.2187Z","iopub.execute_input":"2021-10-14T20:05:54.218965Z","iopub.status.idle":"2021-10-14T20:05:54.232406Z","shell.execute_reply.started":"2021-10-14T20:05:54.21893Z","shell.execute_reply":"2021-10-14T20:05:54.230507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_augmented.to_csv(\"Train_augmented_1000.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T20:05:54.233857Z","iopub.execute_input":"2021-10-14T20:05:54.234218Z","iopub.status.idle":"2021-10-14T20:05:55.732688Z","shell.execute_reply.started":"2021-10-14T20:05:54.23418Z","shell.execute_reply":"2021-10-14T20:05:55.731739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5) :\n    print(f\"We have {df_augmented[df_augmented['type']==i].nunique()} distinc samples for class {i}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-14T20:05:55.734599Z","iopub.execute_input":"2021-10-14T20:05:55.735016Z","iopub.status.idle":"2021-10-14T20:05:55.867435Z","shell.execute_reply.started":"2021-10-14T20:05:55.734977Z","shell.execute_reply":"2021-10-14T20:05:55.86643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}