{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing the data from the drive","metadata":{}},{"cell_type":"code","source":"! pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown \nurl = 'https://drive.google.com/uc?export=download&id=1mykSFmHt-DXpobTQPlmweF8IqVwFtA4v' \noutput = 'GBV.zip'\ngdown.download(url, output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! unzip GBV.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport transformers\nimport torch \nimport torch.nn as nn\nimport warnings\n\nimport io\n\nimport tensorflow as tf\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:35.244732Z","iopub.execute_input":"2021-10-22T05:58:35.245182Z","iopub.status.idle":"2021-10-22T05:58:38.13069Z","shell.execute_reply.started":"2021-10-22T05:58:35.245067Z","shell.execute_reply":"2021-10-22T05:58:38.130026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing the Data\n","metadata":{}},{"cell_type":"code","source":"data_file = 'synoym_30000.csv'\ntest_data_file = \"test_data.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:39.674717Z","iopub.execute_input":"2021-10-22T05:58:39.675608Z","iopub.status.idle":"2021-10-22T05:58:39.680117Z","shell.execute_reply.started":"2021-10-22T05:58:39.675561Z","shell.execute_reply":"2021-10-22T05:58:39.678989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(data_file)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:39.929767Z","iopub.execute_input":"2021-10-22T05:58:39.930039Z","iopub.status.idle":"2021-10-22T05:58:40.397753Z","shell.execute_reply.started":"2021-10-22T05:58:39.929987Z","shell.execute_reply":"2021-10-22T05:58:40.397028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:40.399233Z","iopub.execute_input":"2021-10-22T05:58:40.399521Z","iopub.status.idle":"2021-10-22T05:58:40.413717Z","shell.execute_reply.started":"2021-10-22T05:58:40.399488Z","shell.execute_reply":"2021-10-22T05:58:40.412926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(test_data_file)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:40.414989Z","iopub.execute_input":"2021-10-22T05:58:40.415334Z","iopub.status.idle":"2021-10-22T05:58:40.478089Z","shell.execute_reply.started":"2021-10-22T05:58:40.415294Z","shell.execute_reply":"2021-10-22T05:58:40.477292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing duplicates","metadata":{}},{"cell_type":"code","source":"tweets  = []\ntypes = []\nfor i in range(5):\n    df_tmp = df[df[\"type\"]==i][\"tweet\"].unique()\n    for x in df_tmp:\n        tweets.append(x)\n        types.append(i)\nunique_df = pd.DataFrame({\"tweet\":tweets, \"type\":types})\nunique_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:41.277377Z","iopub.execute_input":"2021-10-22T05:58:41.277649Z","iopub.status.idle":"2021-10-22T05:58:41.583796Z","shell.execute_reply.started":"2021-10-22T05:58:41.277619Z","shell.execute_reply":"2021-10-22T05:58:41.583103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5) :\n    print(f\"We have {unique_df[unique_df['type']==i].nunique()} distinc samples for class {i}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:41.585203Z","iopub.execute_input":"2021-10-22T05:58:41.58609Z","iopub.status.idle":"2021-10-22T05:58:41.752468Z","shell.execute_reply.started":"2021-10-22T05:58:41.586051Z","shell.execute_reply":"2021-10-22T05:58:41.750925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5) :\n    print(f\"We have {len(unique_df[unique_df['type']==i])}  samples for class {i}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:41.754092Z","iopub.execute_input":"2021-10-22T05:58:41.754952Z","iopub.status.idle":"2021-10-22T05:58:41.776119Z","shell.execute_reply.started":"2021-10-22T05:58:41.754913Z","shell.execute_reply":"2021-10-22T05:58:41.77539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_df.to_csv(\"Train_data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:41.946722Z","iopub.execute_input":"2021-10-22T05:58:41.94732Z","iopub.status.idle":"2021-10-22T05:58:43.712923Z","shell.execute_reply.started":"2021-10-22T05:58:41.947265Z","shell.execute_reply":"2021-10-22T05:58:43.712042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_data_file = \"Train_data.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:43.714749Z","iopub.execute_input":"2021-10-22T05:58:43.715218Z","iopub.status.idle":"2021-10-22T05:58:43.71927Z","shell.execute_reply.started":"2021-10-22T05:58:43.715178Z","shell.execute_reply":"2021-10-22T05:58:43.718094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(Train_data_file)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:43.720714Z","iopub.execute_input":"2021-10-22T05:58:43.721343Z","iopub.status.idle":"2021-10-22T05:58:44.164759Z","shell.execute_reply.started":"2021-10-22T05:58:43.721302Z","shell.execute_reply":"2021-10-22T05:58:44.164015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"We have {len(df)} Training samples\")\nprint(f\"We have {len(test_df)} Test samples\")\nprint(f\"We have {df['type'].nunique()} class\")","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:44.16695Z","iopub.execute_input":"2021-10-22T05:58:44.167237Z","iopub.status.idle":"2021-10-22T05:58:44.177652Z","shell.execute_reply.started":"2021-10-22T05:58:44.167202Z","shell.execute_reply":"2021-10-22T05:58:44.176745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = df['type'].unique()\nfor c in classes : \n    print(c)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:44.179138Z","iopub.execute_input":"2021-10-22T05:58:44.179809Z","iopub.status.idle":"2021-10-22T05:58:44.188613Z","shell.execute_reply.started":"2021-10-22T05:58:44.179771Z","shell.execute_reply":"2021-10-22T05:58:44.187749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Class distribution","metadata":{}},{"cell_type":"code","source":"sns.countplot(df[\"type\"])","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:44.385716Z","iopub.execute_input":"2021-10-22T05:58:44.385972Z","iopub.status.idle":"2021-10-22T05:58:44.642792Z","shell.execute_reply.started":"2021-10-22T05:58:44.385946Z","shell.execute_reply":"2021-10-22T05:58:44.641899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"char_count\"] = df[\"tweet\"].apply(len)\ndf[\"word_count\"] = df[\"tweet\"].apply(lambda x:len(list(x.split())))","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:44.644383Z","iopub.execute_input":"2021-10-22T05:58:44.644716Z","iopub.status.idle":"2021-10-22T05:58:45.20546Z","shell.execute_reply.started":"2021-10-22T05:58:44.644677Z","shell.execute_reply":"2021-10-22T05:58:45.20468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.violinplot(x=\"type\", y=\"word_count\", data=df)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:45.206939Z","iopub.execute_input":"2021-10-22T05:58:45.207204Z","iopub.status.idle":"2021-10-22T05:58:46.263651Z","shell.execute_reply.started":"2021-10-22T05:58:45.207169Z","shell.execute_reply":"2021-10-22T05:58:46.262916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.violinplot(x=\"type\", y=\"char_count\", data=df)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:46.264942Z","iopub.execute_input":"2021-10-22T05:58:46.265353Z","iopub.status.idle":"2021-10-22T05:58:47.384909Z","shell.execute_reply.started":"2021-10-22T05:58:46.265314Z","shell.execute_reply":"2021-10-22T05:58:47.384251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idtoclass = ['sexual_violence', 'Physical_violence', 'emotional_violence',\n       'Harmful_Traditional_practice', 'economic_violence']\nclasstoid = {idtoclass[i]:i for i in range(len(idtoclass))}\nprint(idtoclass)\nprint(classtoid)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:47.386635Z","iopub.execute_input":"2021-10-22T05:58:47.387051Z","iopub.status.idle":"2021-10-22T05:58:47.393977Z","shell.execute_reply.started":"2021-10-22T05:58:47.387014Z","shell.execute_reply":"2021-10-22T05:58:47.393275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:58:47.395539Z","iopub.execute_input":"2021-10-22T05:58:47.396572Z","iopub.status.idle":"2021-10-22T05:58:47.41226Z","shell.execute_reply.started":"2021-10-22T05:58:47.396536Z","shell.execute_reply":"2021-10-22T05:58:47.411208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(Train_data_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\nnltk.download(\"punkt\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(Train_data_file)\n\nX_train , X_val, y_train, y_val= train_test_split(df[\"tweet\"].values,df[\"type\"].values ,train_size=0.9,stratify = df[\"type\"].values, random_state=0)\nprint(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(y_val)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvt = CountVectorizer(\n    tokenizer = word_tokenize,\n    token_pattern=None\n)\n\ncvt.fit(X_train)\nXtrain_cvt = cvt.transform(X_train)\nXval_cvt = cvt.transform(X_val)\n\n\nifidf = TfidfVectorizer(\n    tokenizer = word_tokenize,\n    token_pattern=None\n)\n\nifidf.fit(X_train)\nXtrain_tfidf = ifidf.transform(X_train)\nXval_tfidf = ifidf.transform(X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_components = 10000\nPCA_cvt = TruncatedSVD(n_components)\nPCA_cvt.fit(Xtrain_cvt)\nXtrain_cvt_reduced = PCA_cvt.transform(Xtrain_cvt)\nXval_cvt_reduced = PCA_cvt.transform(Xval_cvt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic regression with count vectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n\n\nlr = LogisticRegression(max_iter=200, random_state=42)\n\n\nlr.fit(Xtrain_cvt, y_train)\n\npreds = lr.predict(Xval_cvt)\n\naccuracy = metrics.accuracy_score(y_val, preds)\n\ntrain_preds = lr.predict(Xtrain_cvt)\ntrain_acc = metrics.accuracy_score(y_train, train_preds)\n\nprint(f\"Train Accuracy : {train_acc} - Validation Accuracy : {accuracy}\")\n\nprint(\"Train Confusion matrix\")\nprint(metrics.confusion_matrix(y_train, train_preds))\n\nprint(\"Validation Confusion matrix\")\nprint(metrics.confusion_matrix(y_val, preds))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_data.csv\")\nXtest = cvt.transform(test_df.tweet)\npreds = lr.predict(Xtest)\nsubmission = pd.read_csv(\"SampleSubmission.csv\")\nsubmission[\"type\"] = preds\nsubmission[\"type\"] = submission[\"type\"].apply(lambda x: idtoclass[int(x)])\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training multiple LogRrg models and then take the majority probability\n\nDidn't work as well as one Log Reg","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nnb_models = 3\n\n\nlrs = [LogisticRegression(max_iter=100, random_state=i, solver = \"liblinear\") for i in range(nb_models)] \n\nfor lr in lrs:\n    lr.fit(Xtrain_cvt, y_train)\n\n\npreds_l = [lr.predict_proba(Xval_cvt) for lr in lrs]\npreds_l = np.array(preds_l)\n\npreds_l = np.sum(preds_l, axis=0)/nb_models\n\npreds = np.argmax(preds_l, axis=1)\naccuracy = metrics.accuracy_score(y_val, preds)\n\nprint(f\"Accuracy : {accuracy}\")\n\nprint(metrics.confusion_matrix(y_val, preds))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_data.csv\")\nXtest = ifidf.transform(test_df.tweet)\npreds_l = [lr.predict_proba(Xtest) for lr in lrs]\npreds_l = np.array(preds_l)\n\npreds_l = np.sum(preds_l, axis=0)/nb_models\n\npreds = np.argmax(preds_l, axis=1)\nsubmission = pd.read_csv(\"SampleSubmission.csv\")\nsubmission[\"type\"] = preds\nsubmission[\"type\"] = submission[\"type\"].apply(lambda x: idtoclass[int(x)])\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic regression with TF-IDF\nWorse than Log reg with CVT","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n\nlr = LogisticRegression(max_iter=200, random_state=42)\n\nlr.fit(Xtrain_tfidf, y_train)\n\npreds = lr.predict(Xval_tfidf)\n\naccuracy = metrics.accuracy_score(y_val, preds)\n\nprint(f\"Accuracy : {accuracy}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_data.csv\")\nXtest = ifidf.transform(test_df.tweet)\npreds = lr.predict(Xtest)\nsubmission = pd.read_csv(\"SampleSubmission.csv\")\nsubmission[\"type\"] = preds\nsubmission[\"type\"] = submission[\"type\"].apply(lambda x: idtoclass[int(x)])\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random forest with count vectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=250, max_depth=30)\n\nrf.fit(Xtrain_cvt, y_train)\n\npreds = rf.predict(Xval_cvt)\n\naccuracy = metrics.accuracy_score(y_val, preds)\n\nprint(f\"Accuracy : {accuracy}\")\n\nprint(metrics.confusion_matrix(y_val, preds))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_data.csv\")\nXtest = ifidf.transform(test_df.tweet)\npreds = rf.predict(Xtest)\nsubmission = pd.read_csv(\"SampleSubmission.csv\")\nsubmission[\"type\"] = preds\nsubmission[\"type\"] = submission[\"type\"].apply(lambda x: idtoclass[int(x)])\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deep learning approach","metadata":{}},{"cell_type":"code","source":"! wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n! unzip crawl-300d-2M.vec.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_embeddings(word_index, embedding_file, vector_length=300):\n  max_features = len(word_index) + 1\n  words_to_find = list(word_index.keys())\n  more_words_to_find = []\n\n  for wtf in words_to_find:\n    more_words_to_find.append(wtf)\n    more_words_to_find.append(str(wtf).capitalize())\n\n  more_words_to_find = set(more_words_to_find)\n\n  def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\n  embeddings_index = dict(\n    get_coefs(*o.strip().split(\" \"))\n    for o in open(embedding_file)\n    if o.split(\" \")[0]\n    in more_words_to_find\n    and len(o) > 100\n  )\n  embedding_matrix = np.zeros((max_features, vector_length))\n\n  for word, i in word_index.items():\n    if i >= max_features:\n      continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is None:\n      embedding_vector = embeddings_index.get(\n        str(word).capitalize()\n      )\n    if embedding_vector is None:\n      embedding_vector = embeddings_index.get(\n        str(word).upper()\n      )\n    if (embedding_vector is not None\n      and len(embedding_vector) == vector_length):\n      embedding_matrix[i] = embedding_vector\n  return embedding_matrix\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GBVDataset:\n  def __init__(self, tweets, targets = None, kind = \"train\"):\n    self.tweets = tweets\n    self.kind = kind\n    if kind == \"train\" :\n      self.target = targets\n\n  def __len__(self):\n    return len(self.tweets)\n\n  def __getitem__(self, item):\n\n    tweet = self.tweets[item, :]\n    if self.kind == \"train\" :\n      target = self.target[item]\n      return {\n          \"tweet\" : torch.tensor(tweet, dtype = torch.long),\n          \"target\" : torch.tensor(target, dtype=torch.float)\n      }\n    else : \n      return {\n          \"tweet\" : torch.tensor(tweet, dtype = torch.long),\n      }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MEAN(nn.Module):\n  def __init__(self, embedding_matrix):\n    super(MEAN, self).__init__()\n    num_words = embedding_matrix.shape[0]\n    embed_dim = embedding_matrix.shape[1]\n\n    self.embedding = nn.Embedding(\n        num_embeddings = num_words,\n        embedding_dim = embed_dim\n    )\n\n    self.embedding.weight.requires_grad=False\n\n    self.out = nn.Linear(300, 5)\n\n  def forward(self, x):\n    x = self.embedding(x)\n    \n    out = torch.mean(x, axis=1)\n    \n    out = self.out(out)\n\n    return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n  def __init__(self, embedding_matrix):\n    super(LSTM, self).__init__()\n    num_words = embedding_matrix.shape[0]\n    embed_dim = embedding_matrix.shape[1]\n\n    self.embedding = nn.Embedding(\n        num_embeddings = num_words,\n        embedding_dim = embed_dim\n    )\n\n    self.embedding.weight.requires_grad=False\n\n    self.lstm = nn.LSTM(\n        embed_dim, \n        128, \n        bidirectional=True, \n        batch_first = True\n    )\n\n    self.out = nn.Linear(512, 5)\n\n  def forward(self, x):\n    x = self.embedding(x)\n    \n    x, _ = self.lstm(x)\n\n    avg_pool = torch.mean(x,1)\n    max_pool , _ = torch.max(x,1)\n\n    out = torch.cat((avg_pool , max_pool),1)\n\n    out = self.out(out)\n\n    return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, model , optimizer, device):\n    model.train()\n    fullloss  = 0.0\n    nb = 0\n    for data in data_loader:\n        tweets = data[\"tweet\"]\n        targets = data[\"target\"]\n\n        tweets = tweets.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n\n        optimizer.zero_grad()\n\n        predictions = model(tweets)\n        loss = nn.CrossEntropyLoss()(\n            predictions, \n            targets.long()\n        )\n        fullloss += loss.item()*tweets.shape[0]\n        nb += tweets.shape[0]\n\n        loss.backward()\n        optimizer.step()\n    return fullloss/nb\n\ndef evaluate(data_loader, model , device):\n    final_predictions = []\n    final_targets = []\n\n    model.eval()\n\n    with torch.no_grad():\n        for data in data_loader : \n            tweets = data[\"tweet\"]\n            targets = data[\"target\"]\n\n            tweets = tweets.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            predictions = model(tweets)\n\n            predictions = predictions.cpu().numpy().tolist()\n            targets = data[\"target\"].cpu().numpy().tolist()\n\n            final_predictions.extend(predictions)\n            final_targets.extend(targets)\n    return final_predictions, final_targets\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\nEPOCHS = 10\n\ndf = pd.read_csv(Train_data_file)\n\nxtrain , xtest, ytrain, ytest = train_test_split(df.tweet.values, df.type.values, test_size=0.3, random_state=42, stratify = df.type.values)\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(df.tweet.values.tolist())\n\nxtrain = tokenizer.texts_to_sequences(xtrain)\n\nxtest = tokenizer.texts_to_sequences(xtest)\n\nxtrain = tf.keras.preprocessing.sequence.pad_sequences(\n    xtrain, maxlen=MAX_LEN\n)\n\nxtest = tf.keras.preprocessing.sequence.pad_sequences(\n    xtest, maxlen=MAX_LEN\n)\n\ntrain_dataset = GBVDataset(\n    tweets = xtrain, \n    targets = ytrain\n)\n\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset, \n    batch_size = TRAIN_BATCH_SIZE,\n    num_workers = 2\n)\n\nvalid_dataset = GBVDataset(\n    tweets = xtest, \n    targets = ytest\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset, \n    batch_size = VALID_BATCH_SIZE,\n    num_workers = 1\n)\n\nprint(\"Loading embeddings\")\n\nembedding_matrix = load_embeddings(tokenizer.word_index,\"crawl-300d-2M.vec\")\n\ndevice = torch.device(\"cuda\")\n\nmodel = LSTM(embedding_matrix)\n\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3,gamma=0.1)\n\nprint(\"Training Model\")\n\nbest_accuracy = 0\n\nearly_stopping_counter = 0\n\nbest_model_dict = model.state_dict()\n\n\nfor epoch in range(EPOCHS):\n    loss = train(train_data_loader, model , optimizer, device)\n    scheduler.step()\n    outputs, targets = evaluate(\n      valid_data_loader, model , device\n    )\n\n    outputs_target = np.argmax(np.array(outputs), axis=1)\n\n    accuracy = metrics.accuracy_score(targets, outputs_target)\n\n\n    print(\n      f\"Epoch : {epoch},loss = {loss}, Accuracy Score = {accuracy}\"\n    )\n\n    if accuracy > best_accuracy : \n        best_model_dict = model.state_dict()\n        best_accuracy = accuracy\n    else:\n        early_stopping_counter+=1\n\n    if early_stopping_counter>2:\n        break\n\ntorch.save(best_model_dict,f\"model.pt\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(best_model_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_pred(model , data_loader, device):\n    final_predictions = []\n\n    model.eval()\n\n    with torch.no_grad():\n        for data in data_loader : \n            tweets = data[\"tweet\"]\n\n            tweets = tweets.to(device, dtype=torch.long)\n\n            predictions = model(tweets)\n\n            predictions = predictions.cpu().numpy().tolist()\n\n            final_predictions.extend(predictions)\n    return np.argmax(final_predictions,1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_data.csv\")\n\nxtest = tokenizer.texts_to_sequences(test_df.tweet.values)\n\nxtest = tf.keras.preprocessing.sequence.pad_sequences(\n    xtest, maxlen=MAX_LEN\n)\n\ntest_dataset = GBVDataset(\n    tweets = xtest, \n    kind=\"test\"\n)\n\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset, \n    batch_size = VALID_BATCH_SIZE,\n    num_workers = 1\n)\n\noutputs = generate_pred(\n    model,test_data_loader, device\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"SampleSubmission.csv\")\nsample_submission[\"type\"] = outputs\nsample_submission[\"type\"] = sample_submission[\"type\"].apply(lambda x: idtoclass[int(x)])\nsample_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformers","metadata":{}},{"cell_type":"code","source":"import transformers","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:59:02.419321Z","iopub.execute_input":"2021-10-22T05:59:02.419903Z","iopub.status.idle":"2021-10-22T05:59:02.423832Z","shell.execute_reply.started":"2021-10-22T05:59:02.419858Z","shell.execute_reply":"2021-10-22T05:59:02.422867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config :\n    MAX_LEN = 64\n\n    TRAIN_BATCH_SIZE  = 64\n    VALID_BATCH_SIZE = 64\n\n    EPOCHS = 10\n\n    MODEL_PATH = \"model.bin\"\n\n    TRAINING_FILE = \"Train_data.csv\"\n\n    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n      \"bert-base-uncased\",\n      do_lower_case = True\n    )\n","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:59:02.697447Z","iopub.execute_input":"2021-10-22T05:59:02.698221Z","iopub.status.idle":"2021-10-22T05:59:03.331455Z","shell.execute_reply.started":"2021-10-22T05:59:02.698175Z","shell.execute_reply":"2021-10-22T05:59:03.330645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass BERTDataset:\n  def __init__(self, tweet, target=None):\n    self.tweet = tweet\n    self.target = target\n    self.tokenizer = config.TOKENIZER\n    self.max_len = config.MAX_LEN\n\n  def __len__(self):\n    return len(self.tweet)\n\n  def __getitem__(self, item):\n    tweet = str(self.tweet[item])\n    tweet = \" \".join(tweet.split())\n\n    inputs = self.tokenizer.encode_plus(\n      tweet,\n      None,\n      add_special_tokens=True,\n      max_length=self.max_len,\n      pad_to_max_length=True,\n    )\n\n\n    ids = inputs[\"input_ids\"]\n    mask = inputs[\"attention_mask\"]\n    token_type_ids = inputs[\"token_type_ids\"]\n    \n    if not self.target is None : \n        \n        return {\n            \"ids\":torch.tensor(\n                ids, dtype=torch.long\n                ),\n            \"mask\" : torch.tensor(\n                mask, dtype=torch.long\n            ), \n            \"token_type_ids\" : torch.tensor(\n                token_type_ids, dtype = torch.long\n            ),\n            \"targets\" : torch.tensor(\n                self.target[item],\n                dtype = torch.float\n            )\n        }\n    else :\n        return {\n            \"ids\":torch.tensor(\n                ids, dtype=torch.long\n                ),\n            \"mask\" : torch.tensor(\n                mask, dtype=torch.long\n            ), \n            \"token_type_ids\" : torch.tensor(\n                token_type_ids, dtype = torch.long\n            )\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:59:07.692027Z","iopub.execute_input":"2021-10-22T05:59:07.692811Z","iopub.status.idle":"2021-10-22T05:59:07.703568Z","shell.execute_reply.started":"2021-10-22T05:59:07.692771Z","shell.execute_reply":"2021-10-22T05:59:07.702464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass BERTBaseUncased(nn.Module):\n  def __init__(self):\n    super(BERTBaseUncased, self).__init__()\n    self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n    self.bert_drop = nn.Dropout(0.3)\n\n    self.out = nn.Linear(768, 5)\n\n  def forward(self, ids, mask , token_type_ids):\n\n    _, o2 = self.bert(\n        ids, \n        attention_mask=mask, \n        token_type_ids = token_type_ids, \n        return_dict=False\n    )\n\n    bo = self.bert_drop(o2)\n\n    output = self.out(bo)\n\n    return output","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:59:08.698196Z","iopub.execute_input":"2021-10-22T05:59:08.698917Z","iopub.status.idle":"2021-10-22T05:59:08.705388Z","shell.execute_reply.started":"2021-10-22T05:59:08.698879Z","shell.execute_reply":"2021-10-22T05:59:08.704502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.CrossEntropyLoss()(outputs, targets.long())","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:59:09.794786Z","iopub.execute_input":"2021-10-22T05:59:09.797714Z","iopub.status.idle":"2021-10-22T05:59:09.802532Z","shell.execute_reply.started":"2021-10-22T05:59:09.797673Z","shell.execute_reply":"2021-10-22T05:59:09.801701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm_notebook, tqdm\ndef train_fn(data_loader, model , optimizer, device, scheduler):\n    model.train()\n    for d in tqdm(data_loader) : \n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n\n\n        ids = ids.to(device, dtype=torch.long)\n\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n        mask = mask.to(device, dtype=torch.long)\n\n        targets = targets.to(device, dtype=torch.float)\n\n\n        optimizer.zero_grad()\n\n        outputs = model(\n            ids = ids, \n            mask = mask, \n            token_type_ids = token_type_ids\n        )\n\n        loss = loss_fn(outputs, targets)\n\n        loss.backward()\n\n        optimizer.step()\n\n        scheduler.step()\n\ndef eval_fn(data_loader, model , device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    with torch.no_grad():\n        for d in tqdm(data_loader) : \n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n\n            ids = ids.to(device, dtype=torch.long)\n\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n            mask = mask.to(device, dtype=torch.long)\n\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(\n              ids = ids, \n              mask = mask, \n              token_type_ids = token_type_ids\n            )\n            targets = targets.cpu().detach()\n            fin_targets.extend(targets.numpy().tolist())\n\n            outputs = torch.sigmoid(outputs).cpu().detach()\n            fin_outputs.extend(outputs.numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:59:10.408226Z","iopub.execute_input":"2021-10-22T05:59:10.408802Z","iopub.status.idle":"2021-10-22T05:59:10.420291Z","shell.execute_reply.started":"2021-10-22T05:59:10.408763Z","shell.execute_reply":"2021-10-22T05:59:10.419449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\nfrom sklearn import model_selection , metrics\nfrom transformers import get_linear_schedule_with_warmup\n\n\ndfx  = pd.read_csv(config.TRAINING_FILE)\n\n\ndf_train , df_valid = model_selection.train_test_split(\n  dfx, \n  test_size = 0.1, \n  random_state =42,\n  stratify = dfx.type.values\n)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\n\ntrain_dataset = BERTDataset(\n  tweet = df_train.tweet.values, \n  target = df_train.type.values\n)\n\ntrain_data_loader = torch.utils.data.DataLoader(\n  train_dataset, \n  batch_size = config.TRAIN_BATCH_SIZE,\n  num_workers = 4\n)\n\nvalid_dataset = BERTDataset(\n  tweet = df_valid.tweet.values, \n  target = df_valid.type.values\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n  valid_dataset, \n  batch_size = config.VALID_BATCH_SIZE,\n  num_workers = 4\n)\n\ndevice = torch.device(\"cuda\")\n\nmodel = BERTBaseUncased()\nmodel.to(device)\n\nparam_optimizer = list(model.named_parameters())\n\nno_decay = [\"bias\",\"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n {\n     \"params\":[\n               p for n,p in param_optimizer if\n               not any(nd in n for nd in no_decay)\n     ],\n    \"weight_decay\":0.001,\n } ,\n {\n     \"params\":[\n               p for n,p in param_optimizer if\n               any(nd in n for nd in no_decay)\n     ],\n    \"weight_decay\":0.0,\n } \n\n]\n\nnum_train_steps = int(\n  len(df_train)/config.TRAIN_BATCH_SIZE  * config.EPOCHS\n)\n\noptimizer = AdamW(optimizer_parameters, lr=3e-5)\n\nscheduler = get_linear_schedule_with_warmup(\n  optimizer, \n  num_warmup_steps = 0,\n  num_training_steps = num_train_steps\n)\n\nmodel = nn.DataParallel(model)\n\nbest_accuracy = 0\n\nfor epoch in range(config.EPOCHS):\n    train_fn(\n        train_data_loader, model , optimizer, device, scheduler\n    )\n\n    outputs , targets = eval_fn(\n        valid_data_loader, model , device\n    )\n\n    outputs_targets = np.argmax(np.array(outputs), axis=1)\n\n    accuracy = metrics.accuracy_score(targets, outputs_targets)\n\n    print(f\"Accuracy Score = {accuracy}\")\n\n    if accuracy>best_accuracy:\n        torch.save(model.state_dict(), config.MODEL_PATH)\n        best_accuracy = accuracy\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-22T05:59:11.590977Z","iopub.execute_input":"2021-10-22T05:59:11.591461Z","iopub.status.idle":"2021-10-22T06:27:48.090295Z","shell.execute_reply.started":"2021-10-22T05:59:11.591425Z","shell.execute_reply":"2021-10-22T06:27:48.088894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_pred(model , data_loader, device):\n    final_predictions = []\n\n    model.eval()\n\n    with torch.no_grad():\n        for d in tqdm(data_loader) : \n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n\n            ids = ids.to(device, dtype=torch.long)\n\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n            mask = mask.to(device, dtype=torch.long)\n\n            predictions =  model(\n              ids = ids, \n              mask = mask, \n              token_type_ids = token_type_ids\n            )\n\n            predictions = predictions.cpu().detach().numpy().tolist()\n\n            final_predictions.extend(predictions)\n    return np.argmax(final_predictions,1)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T06:27:50.698428Z","iopub.execute_input":"2021-10-22T06:27:50.698933Z","iopub.status.idle":"2021-10-22T06:27:50.70591Z","shell.execute_reply.started":"2021-10-22T06:27:50.698899Z","shell.execute_reply":"2021-10-22T06:27:50.705103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_data.csv\")\n\ntest_dataset = BERTDataset(\n      tweet = test_df.tweet.values, \n    )\n\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset, \n    batch_size = 64,\n    num_workers = 1\n)\n\noutputs = generate_pred(\n    model,test_data_loader, device\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T06:27:50.890509Z","iopub.execute_input":"2021-10-22T06:27:50.891207Z","iopub.status.idle":"2021-10-22T06:28:23.479775Z","shell.execute_reply.started":"2021-10-22T06:27:50.891173Z","shell.execute_reply":"2021-10-22T06:28:23.478986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"SampleSubmission.csv\")\nsample_submission[\"type\"] = outputs\nsample_submission[\"type\"] = sample_submission[\"type\"].apply(lambda x: idtoclass[int(x)])\nsample_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-22T06:28:23.481564Z","iopub.execute_input":"2021-10-22T06:28:23.482421Z","iopub.status.idle":"2021-10-22T06:28:23.543738Z","shell.execute_reply.started":"2021-10-22T06:28:23.482379Z","shell.execute_reply":"2021-10-22T06:28:23.54307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}