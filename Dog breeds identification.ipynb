{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog breeds identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a simple solution for the dog breeds identification kaagle <a href=\"https://www.kaggle.com/c/dog-breed-identification\">competition</a>. \\\\\n",
    "I used VGG16 pretrained weights for features detection then adding to it 2 trainables dense layers  so it would match our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(labels['id'])\n",
    "breeds = list(labels['breed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to open image using its name and path\n",
    "def open_image(name, path):\n",
    "    image = cv.imread(path+name+'.jpg')\n",
    "    image = cv.cvtColor(image,cv.COLOR_BGR2RGB) ## convert it to RGB because cv2 open images as BGR\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(names) # number of samples\n",
    "path='train/' # path of the samples images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function that show 6 random images from the training set\n",
    "def show_samples():\n",
    "    indexes = np.random.randint(l,size=6)\n",
    "    names_to_show = [names[i] for i in list(indexes)]\n",
    "    \n",
    "    fig , axs = plt.subplots(2, 3, figsize=(13,13))\n",
    "    for i in range(6):\n",
    "        image = open_image(names_to_show[i],path)\n",
    "        label = breeds[indexes[i]]\n",
    "        axs[i//3,i%3].set_title(label)\n",
    "        axs[i//3,i%3].imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples\n",
    "show_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving breeds in different folders to be able to work with image data generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='train_images/' # path where to store training set \n",
    "valid_path='valid_images/' # path where to store validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders\n",
    "#for breed in list(set(breeds)):\n",
    "#    os.makedirs(train_path+breed)\n",
    "#    os.makedirs(valid_path+breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(l) # to shuffle the dataset\n",
    "train_size = int(l*(0.85)) ## training set size\n",
    "valid_size = l-train_size ## validation set size\n",
    "train_indexes = perm[:train_size] ## indexes for training set\n",
    "valid_indexes = perm[train_size:] ## indexes for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the images in their corresponding folders\n",
    "def save_images_into_folder():\n",
    "    for i in train_indexes:\n",
    "        image = cv.imread('train/'+names[i]+'.jpg')\n",
    "        cv.imwrite(train_path+breeds[i]+'/'+names[i]+'.jpg',image)\n",
    "    for i in valid_indexes:\n",
    "        image = cv.imread('train/'+names[i]+'.jpg')\n",
    "        cv.imwrite(valid_path+breeds[i]+'/'+names[i]+'.jpg',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_images_into_folder() ## we need to apply this only one time to save the images in the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enum = list(enumerate(list(set(breeds)))) ## generate list of pairs so we can create mapping between breeds and range[0,119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_to_breed = {item[0]:item[1] for item in enum} ## map index to breed\n",
    "breed_to_val ={item[1]:item[0]for item in enum} ## map breed to index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model librairies\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Input\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.models import model_from_json\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = Input(shape=(128, 128, 3)) # define new input shape\n",
    "\n",
    "model = VGG16(include_top = False ,input_tensor=new_input) # load VGG16 model\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False #  make layers of VGG as non trainable \n",
    "\n",
    "flat1 = Flatten()(model.outputs[0])  ## flatten the ouput of the VGG16 model\n",
    "dense1 = Dense(1024, activation='relu')(flat1) ## dense layer\n",
    "outputs = Dense(120, activation='softmax')(dense1) ## output layer\n",
    "my_model = Model(inputs = model.inputs, outputs= outputs) ## define the model\n",
    "my_model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 5 ## number of epochs\n",
    "batch_size = 32 ## batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the train and validation data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, ## to normalize the images\n",
    "        shear_range=0.2, \n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\")\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "        valid_path,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=250,\n",
    "        epochs=nb_epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save the model\n",
    "model_json = my_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "my_model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model which will be in the file \n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to predict the ouput of image given it's name and path\n",
    "def predict_probas(name,path):\n",
    "    image = tf.keras.preprocessing.image.load_img(path+name+'.jpg', target_size=(128,128), interpolation='bilinear')\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    input_arr = np.array([input_arr]) \n",
    "    input_arr/=255\n",
    "    predictions = loaded_model.predict(input_arr)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict_probas(names[0],path)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pred_ids = [] ## to store the ids of the test set\n",
    "predictions = [] ## to store the results of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for filename in glob.glob('test/*.jpg'):\n",
    "    to_pred_ids.append(filename[5:-4])\n",
    "    predictions.append(predict_probas(filename[5:-4],'test/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv') ## read the sample submission to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = sample_submission.columns ## to get the exact order of the columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame() ## the submission dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['id']=to_pred_ids ## set the ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.reshape((-1,120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the values for the other columns\n",
    "for i in range(1,len(cols)):\n",
    "    colname = cols[i];\n",
    "    ind = breed_to_val[colname]\n",
    "    submission[colname]=predictions[:,ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False) ## save the submission into csv without the index as wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
